{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading readme: 100%|██████████| 542/542 [00:00<00:00, 104kB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 471M/471M [00:25<00:00, 18.2MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 60.7M/60.7M [00:02<00:00, 22.0MB/s]\n",
      "\n",
      "\u001b[A\n",
      "Generating train split: 100%|██████████| 342/342 [00:02<00:00, 149.52 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Generating validation split: 100%|██████████| 38/38 [00:00<00:00, 162.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#loading dataset via Hugging Face API\n",
    "ds = load_dataset('ChrisGuarino/cats')\n",
    "\n",
    "#Data Exploration\n",
    "train_data = ds['train']\n",
    "# test_data = ds['test']\n",
    "validation_data = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'labels': ClassLabel(names=['prim', 'rupe', 'notcat'], id=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the image processor from Hugging Face Hub \n",
    "from transformers import ViTImageProcessor\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.7569, -0.7490, -0.6549,  ...,  0.4039,  0.3882,  0.3725],\n",
       "          [-0.7490, -0.7490, -0.6549,  ...,  0.3961,  0.3804,  0.3882],\n",
       "          [-0.7569, -0.7412, -0.6627,  ...,  0.4275,  0.4118,  0.4196],\n",
       "          ...,\n",
       "          [ 0.4980,  0.4980,  0.4902,  ...,  0.7490,  0.7176,  0.6863],\n",
       "          [ 0.5059,  0.4980,  0.4902,  ...,  0.7804,  0.7255,  0.6706],\n",
       "          [ 0.5216,  0.5059,  0.4902,  ...,  0.7569,  0.7255,  0.6706]],\n",
       "\n",
       "         [[-0.7647, -0.7569, -0.6627,  ...,  0.1216,  0.1137,  0.1137],\n",
       "          [-0.7569, -0.7490, -0.6627,  ...,  0.1137,  0.1059,  0.1137],\n",
       "          [-0.7647, -0.7490, -0.6706,  ...,  0.1294,  0.1294,  0.1451],\n",
       "          ...,\n",
       "          [ 0.4353,  0.4353,  0.4275,  ...,  0.6157,  0.5765,  0.5529],\n",
       "          [ 0.4353,  0.4353,  0.4275,  ...,  0.6392,  0.5843,  0.5294],\n",
       "          [ 0.4353,  0.4275,  0.4275,  ...,  0.6157,  0.5765,  0.5373]],\n",
       "\n",
       "         [[-0.7804, -0.7804, -0.6863,  ..., -0.1922, -0.2000, -0.2000],\n",
       "          [-0.7725, -0.7725, -0.6863,  ..., -0.2000, -0.2078, -0.1922],\n",
       "          [-0.7725, -0.7647, -0.6863,  ..., -0.1686, -0.1765, -0.1608],\n",
       "          ...,\n",
       "          [ 0.2549,  0.2549,  0.2471,  ...,  0.4588,  0.4196,  0.3961],\n",
       "          [ 0.2549,  0.2549,  0.2471,  ...,  0.4824,  0.4275,  0.3804],\n",
       "          [ 0.2706,  0.2549,  0.2549,  ...,  0.4667,  0.4275,  0.3882]]]]), 'labels': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_example(example):\n",
    "    inputs = processor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs\n",
    "process_example(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 342\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\",trust_remote_code=True)\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = ds['train'].features['labels'].names\n",
    "# labels = {0: 'prim', 1: 'rupe'}  # Replace with your actual label mapping\n",
    "\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k',\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"model\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=False,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='none',\n",
    "  load_best_model_at_end=True,\n",
    "  save_strategy=\"epoch\"\n",
    ") \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],  # Make sure you have a validation set\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 13/88 [22:33:26<130:08:16, 6246.62s/it]\n",
      " 11%|█▏        | 10/88 [04:44<37:33, 28.89s/it]\n",
      " 11%|█▏        | 10/88 [04:44<37:33, 28.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7764, 'learning_rate': 0.00017727272727272728, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 20/88 [10:22<31:53, 28.14s/it]\n",
      " 23%|██▎       | 20/88 [10:22<31:53, 28.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1941, 'learning_rate': 0.00015454545454545454, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 22/88 [10:55<23:46, 21.61s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      " 25%|██▌       | 22/88 [11:19<23:46, 21.61s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06949551403522491, 'eval_accuracy': 1.0, 'eval_runtime': 23.3963, 'eval_samples_per_second': 1.624, 'eval_steps_per_second': 0.214, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 30/88 [15:38<27:13, 28.17s/it]\n",
      " 34%|███▍      | 30/88 [15:38<27:13, 28.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0688, 'learning_rate': 0.0001318181818181818, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 40/88 [19:56<20:55, 26.16s/it]\n",
      " 45%|████▌     | 40/88 [19:56<20:55, 26.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0528, 'learning_rate': 0.00010909090909090909, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 44/88 [21:21<15:04, 20.56s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      " 50%|█████     | 44/88 [21:44<15:04, 20.56s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.032007306814193726, 'eval_accuracy': 1.0, 'eval_runtime': 23.4714, 'eval_samples_per_second': 1.619, 'eval_steps_per_second': 0.213, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 50/88 [24:56<22:32, 35.59s/it]\n",
      " 57%|█████▋    | 50/88 [24:56<22:32, 35.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0323, 'learning_rate': 8.636363636363637e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 60/88 [29:34<11:59, 25.71s/it]\n",
      " 68%|██████▊   | 60/88 [29:34<11:59, 25.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0262, 'learning_rate': 6.363636363636364e-05, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 66/88 [31:58<07:39, 20.89s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      " 75%|███████▌  | 66/88 [32:22<07:39, 20.89s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02552206628024578, 'eval_accuracy': 1.0, 'eval_runtime': 23.2412, 'eval_samples_per_second': 1.635, 'eval_steps_per_second': 0.215, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 70/88 [34:30<09:28, 31.56s/it]\n",
      " 80%|███████▉  | 70/88 [34:30<09:28, 31.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0246, 'learning_rate': 4.0909090909090915e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 80/88 [38:56<03:36, 27.05s/it]\n",
      " 91%|█████████ | 80/88 [38:56<03:36, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0224, 'learning_rate': 1.8181818181818182e-05, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [42:07<00:00, 20.89s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      "100%|██████████| 88/88 [42:30<00:00, 20.89s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02296990528702736, 'eval_accuracy': 1.0, 'eval_runtime': 23.4419, 'eval_samples_per_second': 1.621, 'eval_steps_per_second': 0.213, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 88/88 [42:33<00:00, 29.02s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2553.5531, 'train_samples_per_second': 0.536, 'train_steps_per_second': 0.034, 'train_loss': 0.1380413117056543, 'epoch': 4.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  train_loss               =      0.138\n",
      "  train_runtime            = 0:42:33.55\n",
      "  train_samples_per_second =      0.536\n",
      "  train_steps_per_second   =      0.034\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/christopherguarino/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "training_args.bin: 100%|██████████| 4.66k/4.66k [00:00<00:00, 15.4kB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model.safetensors: 100%|██████████| 343M/343M [00:13<00:00, 26.0MB/s]\n",
      "Upload 2 LFS files: 100%|██████████| 2/2 [00:13<00:00,  6.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ChrisGuarino/model/commit/069c1515e52554abfbce5f47df49b532c186566f', commit_message='ChrisGuarino/yummy_model', commit_description='', oid='069c1515e52554abfbce5f47df49b532c186566f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"ChrisGuarino/yummy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catcha-qnF346-r-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
