{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#loading dataset via Hugging Face API\n",
    "ds = load_dataset('ChrisGuarino/cats')\n",
    "\n",
    "#Data Exploration\n",
    "train_data = ds['train']\n",
    "# test_data = ds['test']\n",
    "validation_data = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'labels': ClassLabel(names=['prim', 'rupe'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the image processor from Hugging Face Hub \n",
    "from transformers import ViTImageProcessor\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.9216, -0.8824, -0.7725,  ..., -0.8980, -0.8902, -0.8980],\n",
       "          [-0.8824, -0.8431, -0.6627,  ..., -0.8824, -0.8980, -0.8980],\n",
       "          [-0.5765, -0.6078, -0.6000,  ..., -0.8745, -0.8902, -0.8902],\n",
       "          ...,\n",
       "          [-0.5373, -0.6000, -0.5373,  ..., -0.5922, -0.6235, -0.6784],\n",
       "          [-0.5137, -0.5137, -0.5451,  ..., -0.5843, -0.6078, -0.7490],\n",
       "          [-0.6235, -0.4902, -0.6157,  ..., -0.5608, -0.5922, -0.6863]],\n",
       "\n",
       "         [[-0.9765, -0.9529, -0.9059,  ..., -0.9294, -0.9216, -0.9216],\n",
       "          [-0.9608, -0.9373, -0.8902,  ..., -0.9137, -0.9294, -0.9294],\n",
       "          [-0.8588, -0.8902, -0.8980,  ..., -0.9137, -0.9216, -0.9294],\n",
       "          ...,\n",
       "          [-0.7176, -0.7725, -0.7176,  ..., -0.7647, -0.7882, -0.8353],\n",
       "          [-0.6784, -0.6784, -0.7255,  ..., -0.7647, -0.7882, -0.8745],\n",
       "          [-0.7882, -0.6549, -0.7804,  ..., -0.7412, -0.7725, -0.8353]],\n",
       "\n",
       "         [[-1.0000, -0.9922, -0.9373,  ..., -0.9451, -0.9451, -0.9451],\n",
       "          [-0.9843, -0.9843, -0.9137,  ..., -0.9373, -0.9451, -0.9451],\n",
       "          [-0.9294, -0.9373, -0.9216,  ..., -0.9294, -0.9451, -0.9373],\n",
       "          ...,\n",
       "          [-0.8039, -0.8588, -0.8039,  ..., -0.8275, -0.8510, -0.9137],\n",
       "          [-0.7725, -0.7961, -0.8118,  ..., -0.8275, -0.8431, -0.9451],\n",
       "          [-0.8588, -0.7725, -0.8510,  ..., -0.8039, -0.8275, -0.9137]]]]), 'labels': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_example(example):\n",
    "    inputs = processor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs\n",
    "process_example(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 207\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 24\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/15gytv591x59rnw5gjc7dskh0000gn/T/ipykernel_65878/4091357818.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\",trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\",trust_remote_code=True)\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherguarino/anaconda3/envs/env_py3.9/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = ds['train'].features['labels'].names\n",
    "# labels = {0: 'prim', 1: 'rupe'}  # Replace with your actual label mapping\n",
    "\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k',\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"cat_ds\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=False,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='none',\n",
    "  load_best_model_at_end=True,\n",
    "  save_strategy=\"epoch\"\n",
    ") \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],  # Make sure you have a validation set\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 10/52 [02:10<08:42, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5106, 'learning_rate': 0.00016153846153846155, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 13/52 [02:45<07:18, 11.25s/it]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 13/52 [02:55<07:18, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09747540950775146, 'eval_accuracy': 1.0, 'eval_runtime': 10.6319, 'eval_samples_per_second': 2.257, 'eval_steps_per_second': 0.282, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/52 [04:29<06:48, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0781, 'learning_rate': 0.0001230769230769231, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/52 [05:37<04:39, 10.75s/it]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/52 [05:47<04:39, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02915300987660885, 'eval_accuracy': 1.0, 'eval_runtime': 10.2096, 'eval_samples_per_second': 2.351, 'eval_steps_per_second': 0.294, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/52 [06:44<05:05, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.035, 'learning_rate': 8.461538461538461e-05, 'epoch': 2.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 39/52 [08:29<02:24, 11.12s/it]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 39/52 [08:40<02:24, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.015924831852316856, 'eval_accuracy': 1.0, 'eval_runtime': 10.3279, 'eval_samples_per_second': 2.324, 'eval_steps_per_second': 0.29, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 40/52 [08:57<03:14, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0171, 'learning_rate': 4.615384615384616e-05, 'epoch': 3.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 50/52 [11:01<00:24, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'learning_rate': 7.692307692307694e-06, 'epoch': 3.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [11:31<00:00, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.013783477246761322, 'eval_accuracy': 1.0, 'eval_runtime': 10.0744, 'eval_samples_per_second': 2.382, 'eval_steps_per_second': 0.298, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [11:33<00:00, 13.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 693.3707, 'train_samples_per_second': 1.194, 'train_steps_per_second': 0.075, 'train_loss': 0.12617188778061134, 'epoch': 4.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  train_loss               =     0.1262\n",
      "  train_runtime            = 0:11:33.37\n",
      "  train_samples_per_second =      1.194\n",
      "  train_steps_per_second   =      0.075\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ChrisGuarino/cat_ds/commit/e01cac9b9a43cbcda3d5fbc6147ef2a2deacec83', commit_message='ChrisGuarino/cats', commit_description='', oid='e01cac9b9a43cbcda3d5fbc6147ef2a2deacec83', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"ChrisGuarino/cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catcha-qnF346-r-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
